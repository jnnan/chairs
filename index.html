<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="description" content="">
        <meta name="keywords" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>CHAIRS</title>
        
        
        <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
      
        <link rel="stylesheet" href="./static/css/bulma.min.css">
        <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <!-- Bootstrap core CSS -->
        <!--link href="bootstrap.min.css" rel="stylesheet"-->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
              integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
        <link rel="stylesheet" href="./static/css/index.css">

        <script src="https://kit.fontawesome.com/d383128d10.js" crossorigin="anonymous"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <script src="./static/js/bulma-carousel.min.js"></script>
        <script src="./static/js/bulma-slider.min.js"></script>
        <script src="./static/js/index.js"></script>
        <script src="./static/js/main.js"></script>
      </head>
<body>
    <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <h1 class="title is-1 publication-title">Full-Body Articulated Human-Object Interaction</h1>
                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <a href="https://jnnan.github.io/">Nan Jiang<sup>1,2*+</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="http://tengyu.ai/">Tengyu Liu<sup>2*</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="">Zhexuan Cao<sup>3+</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="https://jiemingcui.github.io/">Jieming Cui<sup>1,2+</sup></a>&emsp;</span>
		  <span class="author-block">
                    <a href="">Zhiyuan Zhang<sup>2,3+</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="https://yixchen.github.io/">Yixin Chen<sup>2</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="https://hughw19.github.io/">He Wang<sup>4</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="https://yzhu.io"><i class="fa-solid fa-envelope"></i>Yixin Zhu<sup>5</sup></a>&emsp;</span>
                  <span class="author-block">
                    <a href="https://siyuanhuang.com/"><i class="fa-solid fa-envelope"></i>Siyuan Huang<sup>2</sup></a>&emsp;</span>
      
                </div>
      		<br/>
                <div class="is-size-5 publication-authors">
                  <span class="author-block"><sup>1</sup>School of Intelligence Science and Technology, Peking University</span>&emsp;
                  <span class="author-block"><sup>2</sup>Beijing Institute of General Artificial Intelligence (BIGAI)</span>&emsp;
                  <span class="author-block"><sup>3</sup>Department of Automation, Tsinghua University</span><br/>
		  <span class="author-block"><sup>4</sup>Center on Frontiers of Computing Studies, Peking University</span>&emsp;
		  <span class="author-block"><sup>5</sup>Institute for Artificial Intelligence, Peking University</span><br/>
                  <span class="author-block"><sup>*</sup>Equal contributors</span>&emsp;
                  <span class="author-block"><sup>+</sup>Work done during internship at BIGAI</span><br/>
                </div>
      
                <div class="column has-text-centered">
                  <div class="publication-links">
                    <!-- PDF Link. -->
                    <span class="link-block">
                      <a href="https://arxiv.org/pdf/2212.10621.pdf"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    
                    <!-- <span class="link-block">
                      <a href="./static/paper/supplementary.pdf"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Supplementary</span>
                      </a>
                    </span> -->
                    <!-- Video Link. -->
                    <span class="link-block">
                      <a href="https://forms.gle/t4SjmJS4RPx7AFvFA"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fas fa-download"></i>
                        </span>
                        <span>Data</span>
                      </a>
                    </span>
      
                    
      
                    <!-- Code Link. -->
                    <span class="link-block">
                      <a href="https://github.com/jnnan/chairs"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                    </span>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
    <!-- <script src="./scripts.js" type="module"></script> -->
    <section class="section">
    <div class="container is-max-desktop">
    	<div class="columns is-centered has-text-centered">
    		 Drag to move your view around
    	</div>
        <div class="columns is-centered has-text-centered">
            <canvas class="webgl" style="height: 50%; width: 50%;"></canvas>
        </div>
    </div>
    </section>

    

    <section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
            <p>
                Fine-grained capturing of 3D HOI boosts human activity understanding and facilitates downstream visual tasks, including action recognition, holistic scene reconstruction, and human motion synthesis. Despite its significance, existing works mostly assume that humans interact with rigid objects using only a few body parts, limiting their scope. In this paper, we address the challenging problem of f-AHOI, wherein the whole human bodies interact with articulated objects, whose parts are connected by movable joints. We present CHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hours of versatile interactions between 46 participants and 74 articulated and rigid sittable objects. CHAIRS provides 3D meshes of both humans and articulated objects during the entire interactive process, as well as realistic and physically plausible full-body interactions. We show the value of CHAIRS with object pose estimation. By learning the geometrical relationships in HOI, we devise the very first model that leverage human pose estimation to tackle the estimation of articulated object poses and shapes during whole-body interactions. Given an image and an estimated human pose, our model first reconstructs the pose and shape of the object, then optimizes the reconstruction according to a learned interaction prior. Under both evaluation settings (e.g., with or without the knowledge of objects' geometries/structures), our model significantly outperforms baselines. We hope CHAIRS will promote the community towards finer-grained interaction understanding. We will make the data/code publicly available.
    
            </p>
            </div>
        </div>
        </div>
    </div>

    <br/>
    <br/>
    <br/>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img src="static/images/teaser.png" height="100%" width="100%"/><br/><br/>
                <h2 class="subtitle has-text-justified">
                    <span class="dnerf" margin-bottom: 0px;></span> Examples of the proposed A-HOI datasetâ€”CHAIRS contains fine-grained interactions between 46
                    participants and 74 sittable objects with drastically different kinematic structures, providing multi-view RGB-D
                    sequence inputs and ground-truth 3D mesh of humans and articulated objects for over 16.2 hours of recordings.
                </h2>
            </div>
        </div>
    </section>

    >
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <center>
                    <h2 class="title is-4"><span style="background-color:  #424949; color: white; border-radius: 7px; padding-left:10px; padding-right:10px; padding-top:5px; padding-bottom:5px;"> Method Overview</span></h2>
                    <img src="static/images/method.png" height="100%" width="100%"/><br/><br/>
                    <h2 class="subtitle has-text-justified">
                        <span class="dnerf" margin-bottom: 0px;></span> The overall architecture of our model - The reconstruction model uses the predicted voxelized human to guide the pose estimation of the interacting object. We further regress the root 6D pose of the object using the image feature and the SMPL-X parameters. We utilize both predictions and an interaction prior to optimize the final estimated pose.
                    </h2>
                </center>
            </div>
        </div>
    </section>

    <br/>
    <br/>

    <section class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
        <center>
            <h2 class="title is-4"><span style="background-color:  #424949; color: white; border-radius: 7px; padding-left:10px; padding-right:10px; padding-top:5px; padding-bottom:5px;"> Dataset Clips</span></h2><br/>
            <table align=center width="1000px" style="background-color:white;">

            <!-- <tr style="height: 20px;">
                <td colspan="3"></td>
            </tr> -->
            <tr style="height: 20px;">
                <td>
                    <div class="publication-video">
                        <video src="./static/images/cmb03.mp4?autoplay=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen controls loop autoplay muted></video> 
                    </div> 
                </td>
                <td>
                    <p style="font-size:20px;"><b><center>&emsp;&emsp;&emsp;</center></p>
                </td>
                <td>
                    <div class="publication-video">
                        <video src="./static/images/cmb02.mp4?autoplay=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen controls loop autoplay muted></video> 
                    </div> 
                </td>
                
            </tr>

            <tr style="height: 20px;">
                <td>
                    <div class="publication-video">
                        <video src="./static/images/cmb01.mp4?autoplay=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen controls loop autoplay muted></video> 
                    </div> 
                </td>
                <td>
                    <p style="font-size:20px;"><b><center>&emsp;&emsp;&emsp;</center></p>
                </td>
                <td>
                    <div class="publication-video">
                        <video src="./static/images/cmb04.mp4?autoplay=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen controls loop autoplay muted></video> 
                    </div> 
                </td>
                
            </tr>

            
            </table>
        </center>
    </div>

    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
    
    
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <center>
                    <h2 class="title is-4"><span style="background-color:  #424949; color: white; border-radius: 7px; padding-left:10px; padding-right:10px; padding-top:5px; padding-bottom:5px;"> Result Examples</span></h2>
                    <img src="static/images/supp-qual-2x-100.jpg" height="100%" width="100%"/><br/><br/>
                    <h2 class="subtitle has-text-justified">
                        <span class="dnerf" margin-bottom: 0px;></span> The optimization results of our method on CHAIRS dataset. The first line is the original rgb input. The second and third line is the result of our full model with mesh reconstruction and part-level 6D pose estimation. The fourth line is the result of mesh recostruction without knowledge of the object.
                    </h2>
                    <br/>
    <br/>
                    <img src="static/images/supp-wild-2x-100.jpg" height="100%" width="100%"/><br/><br/>
                    <h2 class="subtitle has-text-justified">
                        <span class="dnerf" margin-bottom: 0px;></span> The optimization results of our method on in-the-wild images with a person sitting in a sitable furniture.
                    </h2>
                    <br/>
    <br/>
                    <img src="static/images/supp-behave-2x-100.jpg" height="100%" width="100%"/><br/><br/>
                    <h2 class="subtitle has-text-justified">
                        <span class="dnerf" margin-bottom: 0px;></span> The optimization results of our method on <a href="https://virtualhumans.mpi-inf.mpg.de/behave">BEHAVE</a> dataset.
                    </h2>
                </center>
            </div>
        </div>
    </section>
    

    <br/>
    <br/>
    <br/>
    
    <!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-M4VYMN5FFR"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-M4VYMN5FFR');
	</script>

    
      <script src="./scripts_test.js" type="module"></script>
</body>
</html>
